{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eXtreme Federated Learning guided by Sensitivity (XFL-S)\n",
    "Author: Khaji Sana \n",
    "Description:\n",
    "    XFL-S implements sensitivity-based layer selection in federated learning to \n",
    "    reduce communication overhead while maintaining model accuracy.\n",
    "\"\"\"\n",
    "# IMPORTS\n",
    "import os\n",
    "os.environ[\"RAY_DEDUP_LOGS\"] = \"0\"\n",
    "\n",
    "# Core libraries\n",
    "import flwr\n",
    "import flwr as fl\n",
    "from collections import OrderedDict, defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from flwr_datasets import FederatedDataset\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from flwr_datasets.partitioner import IidPartitioner, PathologicalPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Union, Optional, Callable\n",
    "from flwr.client import NumPyClient, ClientApp\n",
    "from flwr.common import Parameters, ndarrays_to_parameters, parameters_to_ndarrays, Context, Metrics, FitRes, Scalar\n",
    "from flwr.server import ServerApp, ServerAppComponents, ServerConfig\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "import logging\n",
    "from flwr.simulation import run_simulation\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE CONFIGURATION\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {device}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")\n",
    "disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPRODUCIBILITY SETUP\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"Random seeds set for reproducibility\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARIO CONFIGURATIONS\n",
    "\"\"\"\n",
    "Four experimental scenarios are available:\n",
    "1. 6-Layer Model with IID Data Distribution\n",
    "2. 6-Layer Model with Non-IID Data Distribution\n",
    "3. 10-Layer Model with IID Data Distribution\n",
    "4. 10-Layer Model with Non-IID Data Distribution\n",
    "\"\"\"\n",
    "\n",
    "SCENARIOS = {\n",
    "    \"6_layer_iid\": {\n",
    "        \"name\": \"6-Layer Model with IID Data\",\n",
    "        \"model_type\": \"6_layer\",\n",
    "        \"data_distribution\": \"iid\",\n",
    "        \"num_rounds\": 20,\n",
    "        \"num_clients\": 18,\n",
    "        \"local_epochs\": 40,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"fraction_evaluate\": 1.0,\n",
    "    },\n",
    "    \"6_layer_noniid\": {\n",
    "        \"name\": \"6-Layer Model with Non-IID Data\",\n",
    "        \"model_type\": \"6_layer\",\n",
    "        \"data_distribution\": \"noniid\",\n",
    "        \"num_rounds\": 20,\n",
    "        \"num_clients\": 30,\n",
    "        \"local_epochs\": 50,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"fraction_evaluate\": 1.0,\n",
    "    },\n",
    "    \"10_layer_iid\": {\n",
    "        \"name\": \"10-Layer Model with IID Data\",\n",
    "        \"model_type\": \"10_layer\",\n",
    "        \"data_distribution\": \"iid\",\n",
    "        \"num_rounds\": 20,\n",
    "        \"num_clients\": 18,\n",
    "        \"local_epochs\": 40,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"fraction_evaluate\": 1.0,\n",
    "    },\n",
    "    \"10_layer_noniid\": {\n",
    "        \"name\": \"10-Layer Model with Non-IID Data\",\n",
    "        \"model_type\": \"10_layer\",\n",
    "        \"data_distribution\": \"noniid\",\n",
    "        \"num_rounds\": 20,\n",
    "        \"num_clients\": 30,\n",
    "        \"local_epochs\": 50,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 32,\n",
    "        \"fraction_evaluate\": 1.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCENARIO SELECTION - Choose ONE scenario by uncommenting it\n",
    "# SCENARIO 1: 6-Layer Model with IID Data Distribution\n",
    "# scenario_config = SCENARIOS[\"6_layer_iid\"]\n",
    "\n",
    "# SCENARIO 2: 6-Layer Model with Non-IID Data Distribution\n",
    "# scenario_config = SCENARIOS[\"6_layer_noniid\"]\n",
    "\n",
    "# SCENARIO 3: 10-Layer Model with IID Data Distribution\n",
    "# scenario_config = SCENARIOS[\"10_layer_iid\"]\n",
    "\n",
    "# SCENARIO 4: 10-Layer Model with Non-IID Data Distribution\n",
    "scenario_config = SCENARIOS[\"10_layer_noniid\"]  # <-- Currently active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a41e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT CONFIGURATION PARAMETERS\n",
    "NUM_ROUNDS = scenario_config[\"num_rounds\"]\n",
    "NUM_CLIENTS = scenario_config[\"num_clients\"]\n",
    "LOCAL_EPOCHS = scenario_config[\"local_epochs\"]\n",
    "LEARNING_RATE = scenario_config[\"learning_rate\"]\n",
    "BATCH_SIZE = scenario_config[\"batch_size\"]\n",
    "FRACTION_EVALUATE = scenario_config[\"fraction_evaluate\"]\n",
    "MODEL_TYPE = scenario_config[\"model_type\"]\n",
    "DATA_DISTRIBUTION = scenario_config[\"data_distribution\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"SCENARIO: {scenario_config['name']}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  - Model: {MODEL_TYPE}\")\n",
    "print(f\"  - Data Distribution: {DATA_DISTRIBUTION.upper()}\")\n",
    "print(f\"  - Rounds: {NUM_ROUNDS}\")\n",
    "print(f\"  - Clients: {NUM_CLIENTS}\")\n",
    "print(f\"  - Local Epochs (t): {LOCAL_EPOCHS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5565d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK MODEL DEFINITIONS\n",
    "\n",
    "class Net6Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    6-Layer CNN Model for CIFAR-10\n",
    "    Architecture: 4 Convolutional Layers + 2 Fully Connected Layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net6Layer, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)   # Layer 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # Layer 2\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Layer 3\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1) # Layer 4\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(2 * 2 * 128, 256)  # Layer 5\n",
    "        self.fc2 = nn.Linear(256, 10)           # Layer 6 (output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # 4x4 -> 2x2\n",
    "        x = x.view(-1, 2 * 2 * 128)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class Net10Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    10-Layer CNN Model for CIFAR-10\n",
    "    Architecture: 6 Convolutional Layers + 4 Fully Connected Layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net10Layer, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)    # Layer 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)   # Layer 2\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, 1, 1)   # Layer 3\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, 1, 1)  # Layer 4\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, 1, 1) # Layer 5\n",
    "        self.conv6 = nn.Conv2d(256, 512, 3, 1, 1) # Layer 6\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(1 * 1 * 512, 512)  # Layer 7\n",
    "        self.fc2 = nn.Linear(512, 256)          # Layer 8\n",
    "        self.fc3 = nn.Linear(256, 64)           # Layer 9\n",
    "        self.fc4 = nn.Linear(64, 10)            # Layer 10 (output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
    "        x = self.pool(F.relu(self.conv4(x)))  # 4x4 -> 2x2\n",
    "        x = self.pool(F.relu(self.conv5(x)))  # 2x2 -> 1x1\n",
    "        x = F.relu(self.conv6(x))             # 1x1 -> 1x1 (no pooling)\n",
    "        x = x.view(-1, 1 * 1 * 512)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.fc4(x)\n",
    "\n",
    "def get_model(model_type: str) -> nn.Module:\n",
    "    \"\"\"Factory function to get the appropriate model based on configuration.\"\"\"\n",
    "    if model_type == \"6_layer\":\n",
    "        return Net6Layer()\n",
    "    elif model_type == \"10_layer\":\n",
    "        return Net10Layer()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "\n",
    "print(f\"Neural Network Model defined: {MODEL_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def get_weights(net: nn.Module) -> List[np.ndarray]:\n",
    "    \"\"\"Extract model weights as numpy arrays.\"\"\"\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_weights(net: nn.Module, parameters: List[np.ndarray]) -> None:\n",
    "    \"\"\"Set model weights from numpy arrays.\"\"\"\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1422835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADING FUNCTIONS\n",
    "\n",
    "# Global cache for FederatedDataset\n",
    "_fds = None\n",
    "\n",
    "\n",
    "def load_data_iid(partition_id: int, num_partitions: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    Load IID partitioned CIFAR-10 data.\n",
    "    \n",
    "    Args:\n",
    "        partition_id: Client partition ID\n",
    "        num_partitions: Total number of partitions\n",
    "        batch_size: Batch size for DataLoader\n",
    "    \n",
    "    Returns:\n",
    "        trainloader, testloader: DataLoaders for training and testing\n",
    "    \"\"\"\n",
    "    global _fds\n",
    "    if _fds is None:\n",
    "        partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "        _fds = FederatedDataset(\n",
    "            dataset=\"uoft-cs/cifar10\",\n",
    "            partitioners={\"train\": partitioner},\n",
    "        )\n",
    "    \n",
    "    partition = _fds.load_partition(partition_id)\n",
    "    partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    pytorch_transforms = Compose([\n",
    "        ToTensor(), \n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(\n",
    "        partition_train_test[\"train\"], batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        partition_train_test[\"test\"], batch_size=batch_size\n",
    "    )\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def load_data_noniid(\n",
    "    partition_id: int,\n",
    "    num_partitions: int,\n",
    "    batch_size: int,\n",
    "    num_classes_per_client: int = 4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load Non-IID partitioned CIFAR-10 data using pathological partitioning.\n",
    "    \n",
    "    Args:\n",
    "        partition_id: Client partition ID\n",
    "        num_partitions: Total number of partitions\n",
    "        batch_size: Batch size for DataLoader\n",
    "        num_classes_per_client: Number of classes per client partition\n",
    "    \n",
    "    Returns:\n",
    "        trainloader, testloader: DataLoaders for training and testing\n",
    "    \"\"\"\n",
    "    global _fds\n",
    "    if _fds is None:\n",
    "        partitioner = PathologicalPartitioner(\n",
    "            num_partitions=num_partitions,\n",
    "            partition_by=\"label\",\n",
    "            num_classes_per_partition=num_classes_per_client,\n",
    "            class_assignment_mode=\"deterministic\",\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "        )\n",
    "        _fds = FederatedDataset(\n",
    "            dataset=\"uoft-cs/cifar10\",\n",
    "            partitioners={\"train\": partitioner},\n",
    "        )\n",
    "    \n",
    "    partition = _fds.load_partition(partition_id)\n",
    "    tr_te = partition.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    transforms = Compose([\n",
    "        ToTensor(), \n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    tr_te = tr_te.with_transform(\n",
    "        lambda batch: {\n",
    "            \"img\": [transforms(img) for img in batch[\"img\"]],\n",
    "            \"label\": batch[\"label\"],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    trainloader = DataLoader(tr_te[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "    testloader = DataLoader(tr_te[\"test\"], batch_size=batch_size)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "def load_data(partition_id: int, num_partitions: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    Load data based on the selected data distribution configuration.\n",
    "    \n",
    "    Args:\n",
    "        partition_id: Client partition ID\n",
    "        num_partitions: Total number of partitions\n",
    "        batch_size: Batch size for DataLoader\n",
    "    \n",
    "    Returns:\n",
    "        trainloader, testloader: DataLoaders for training and testing\n",
    "    \"\"\"\n",
    "    if DATA_DISTRIBUTION == \"iid\":\n",
    "        return load_data_iid(partition_id, num_partitions, batch_size)\n",
    "    elif DATA_DISTRIBUTION == \"noniid\":\n",
    "        return load_data_noniid(partition_id, num_partitions, batch_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data distribution: {DATA_DISTRIBUTION}\")\n",
    "\n",
    "\n",
    "print(f\"Data loading functions defined (Distribution: {DATA_DISTRIBUTION.upper()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND TESTING FUNCTIONS\n",
    "\n",
    "def train_with_epoch_tracking(\n",
    "    net: nn.Module,\n",
    "    trainloader: DataLoader,\n",
    "    valloader: DataLoader,\n",
    "    epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: torch.device\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Train the model with epoch-wise parameter tracking for sensitivity calculation.\n",
    "    Args:\n",
    "        net: Neural network model\n",
    "        trainloader: Training data loader\n",
    "        valloader: Validation data loader\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Learning rate\n",
    "        device: Device to train on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing validation metrics and epoch-wise parameters\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    net.train()\n",
    "    \n",
    "    # Store parameters after each epoch for Δ calculation\n",
    "    epoch_parameters = []\n",
    "    \n",
    "    # Store initial parameters (w_0)\n",
    "    initial_params = get_weights(net)\n",
    "    epoch_parameters.append(copy.deepcopy(initial_params))\n",
    "    \n",
    "    # Training loop with epoch tracking\n",
    "    for epoch in range(epochs):\n",
    "        for batch in trainloader:\n",
    "            images = batch[\"img\"]\n",
    "            labels = batch[\"label\"]\n",
    "            optimizer.zero_grad()\n",
    "            criterion(net(images.to(device)), labels.to(device)).backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Store parameters after this epoch (w_e)\n",
    "        current_params = get_weights(net)\n",
    "        epoch_parameters.append(copy.deepcopy(current_params))\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_precision, val_recall, val_f1 = test(net, valloader, device)\n",
    "    \n",
    "    results = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_accuracy\": val_acc,\n",
    "        \"val_precision\": val_precision,\n",
    "        \"val_recall\": val_recall,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"epoch_parameters\": epoch_parameters\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def test(\n",
    "    net: nn.Module,\n",
    "    testloader: DataLoader,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Test the model and compute comprehensive metrics.\n",
    "    \n",
    "    Args:\n",
    "        net: Neural network model\n",
    "        testloader: Test data loader\n",
    "        device: Device to test on\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (loss, accuracy, precision, recall, f1_score)\n",
    "    \"\"\"\n",
    "    net.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, loss = 0, 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = batch[\"img\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    loss = loss / len(testloader)\n",
    "    \n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    return loss, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "print(\"Training and testing functions defined with epoch tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f05f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XFL-S CLIENT IMPLEMENTATION\n",
    "\n",
    "# Configure client-side logging\n",
    "client_logger = logging.getLogger(\"XFLSClient\")\n",
    "client_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class XFLSClient(NumPyClient):\n",
    "    \"\"\"\n",
    "    XFL-S (eXtreme Federated Learning guided by Sensitivity) Client.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        local_epochs: int,\n",
    "        learning_rate: float,\n",
    "        rank: int,\n",
    "        cid: str\n",
    "    ):\n",
    "        self.net = net\n",
    "        self.rank = rank\n",
    "        self.cid = cid\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net.to(self.device)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.local_epochs = local_epochs  # This is 't' in the formula\n",
    "\n",
    "    def calculate_average_local_updates(\n",
    "        self,\n",
    "        epoch_parameters: List[List[np.ndarray]]\n",
    "    ) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate average local updates \n",
    "        \n",
    "        Args:\n",
    "            epoch_parameters: List of parameters after each epoch\n",
    "        \n",
    "        Returns:\n",
    "            List of average local updates for each parameter\n",
    "        \"\"\"\n",
    "        t = self.local_epochs\n",
    "        num_layers = len(epoch_parameters[0])\n",
    "        \n",
    "        # Initialize average updates\n",
    "        delta_t_l = [np.zeros_like(epoch_parameters[0][l]) for l in range(num_layers)]\n",
    "        \n",
    "        # Calculate sum of (w_{e,l}^local - w_{e-1,l}^local) for each epoch e\n",
    "        for e in range(1, t + 1):  # e from 1 to t\n",
    "            w_e = epoch_parameters[e]          # Parameters after epoch e\n",
    "            w_e_minus_1 = epoch_parameters[e - 1]  # Parameters after epoch e-1\n",
    "            \n",
    "            for l in range(num_layers):\n",
    "                # Add (w_{e,l}^local - w_{e-1,l}^local) to the sum\n",
    "                delta_t_l[l] += (w_e[l] - w_e_minus_1[l])\n",
    "        \n",
    "        # Divide by t to get average\n",
    "        for l in range(num_layers):\n",
    "            delta_t_l[l] = delta_t_l[l] / t\n",
    "        \n",
    "        return delta_t_l\n",
    "\n",
    "    def calculate_sensitivity_scores(\n",
    "        self,\n",
    "        delta_t_l: List[np.ndarray],\n",
    "        global_parameters: List[np.ndarray]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        Calculate sensitivity scores\n",
    "        \n",
    "        Args:\n",
    "            delta_t_l: Average local updates\n",
    "            global_parameters: Global model parameters\n",
    "        \n",
    "        Returns:\n",
    "            List of sensitivity scores for each parameter\n",
    "        \"\"\"\n",
    "        sensitivity_scores = []\n",
    "        eps = 1e-12  # Small epsilon to avoid division by zero\n",
    "        \n",
    "        for l in range(len(delta_t_l)):\n",
    "            # Calculate  (L2 norm of average local update)\n",
    "            delta_norm = np.linalg.norm(delta_t_l[l])\n",
    "            \n",
    "            # Calculate (L2 norm of global parameters)\n",
    "            global_norm = np.linalg.norm(global_parameters[l])\n",
    "            \n",
    "            # Calculate sensitivity\n",
    "            sensitivity = delta_norm / (global_norm + eps)\n",
    "            sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        return sensitivity_scores\n",
    "\n",
    "    def calculate_probability_distribution(\n",
    "        self,\n",
    "        sensitivity_scores: List[float]\n",
    "    ) -> Tuple[np.ndarray, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculate probability distribution: \n",
    "        \n",
    "        Args:\n",
    "            sensitivity_scores: Sensitivity scores for each parameter\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (probabilities, layer_sensitivities)\n",
    "        \"\"\"\n",
    "        # Group by layers (each layer has weight + bias, so we average them)\n",
    "        num_layers = len(sensitivity_scores) // 2\n",
    "        layer_sensitivities = []\n",
    "        \n",
    "        for layer_idx in range(num_layers):\n",
    "            weight_sens = sensitivity_scores[2 * layer_idx]      # Weight sensitivity\n",
    "            bias_sens = sensitivity_scores[2 * layer_idx + 1]    # Bias sensitivity\n",
    "            # Average sensitivity for this layer\n",
    "            layer_sensitivity = (weight_sens + bias_sens) / 2\n",
    "            layer_sensitivities.append(layer_sensitivity)\n",
    "        \n",
    "        # Calculate sum of all sensitivities\n",
    "        total_sensitivity = sum(layer_sensitivities)\n",
    "        \n",
    "        # Handle edge case where all sensitivities are zero\n",
    "        if total_sensitivity <= 0 or not np.isfinite(total_sensitivity):\n",
    "            # Uniform distribution as fallback\n",
    "            probabilities = [1.0 / num_layers] * num_layers\n",
    "        else:\n",
    "            # Calculate probabilities \n",
    "            probabilities = [s / total_sensitivity for s in layer_sensitivities]\n",
    "        \n",
    "        # Ensure probabilities sum to 1 (numerical stability)\n",
    "        probabilities = np.array(probabilities)\n",
    "        probabilities = probabilities / probabilities.sum()\n",
    "        \n",
    "        return probabilities, layer_sensitivities\n",
    "\n",
    "    def fit(self, parameters: List[np.ndarray], config: Dict) -> Tuple[List[np.ndarray], int, Dict]:\n",
    "        \"\"\"\n",
    "        Train the model locally and select a layer based on sensitivity.\n",
    "        \n",
    "        Args:\n",
    "            parameters: Global model parameters\n",
    "            config: Configuration dictionary\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (selected_layer_parameters, num_examples, metrics)\n",
    "        \"\"\"\n",
    "        # Set global weights (x_{t,l})\n",
    "        set_weights(self.net, parameters)\n",
    "        global_parameters = get_weights(self.net)\n",
    "        \n",
    "        # Train with epoch tracking\n",
    "        train_results = train_with_epoch_tracking(\n",
    "            self.net,\n",
    "            trainloader=self.trainloader,\n",
    "            valloader=self.valloader,\n",
    "            epochs=self.local_epochs,\n",
    "            learning_rate=self.learning_rate,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Extract epoch parameters for Δ calculation\n",
    "        epoch_parameters = train_results[\"epoch_parameters\"]\n",
    "        \n",
    "        # STEP 1: Calculate average local updates Δ_{t,l}\n",
    "        delta_t_l = self.calculate_average_local_updates(epoch_parameters)\n",
    "        \n",
    "        # STEP 2: Calculate sensitivity scores s_{t,l}\n",
    "        sensitivity_scores = self.calculate_sensitivity_scores(delta_t_l, global_parameters)\n",
    "        \n",
    "        # STEP 3: Calculate probability distribution p_{t,l}\n",
    "        probabilities, layer_sensitivities = self.calculate_probability_distribution(sensitivity_scores)\n",
    "        \n",
    "        # STEP 4: Select layer based on probability distribution\n",
    "        selected_layer_idx = np.random.choice(len(probabilities), p=probabilities)\n",
    "        layer_selected = selected_layer_idx + 1  # Convert to 1-indexed\n",
    "        \n",
    "        round_num = int(config.get(\"round\", 1))\n",
    "        client_logger.info(\n",
    "            f\"[Round {round_num}] Client {self.rank} (CID: {self.cid}) selected layer {layer_selected} \"\n",
    "            f\"with probability {probabilities[selected_layer_idx]:.4f} \"\n",
    "            f\"(sensitivity: {layer_sensitivities[selected_layer_idx]:.6f})\"\n",
    "        )\n",
    "        \n",
    "        # Get final local parameters\n",
    "        final_local_parameters = get_weights(self.net)\n",
    "        \n",
    "        # Package selected layer parameters (weight + bias)\n",
    "        weight_idx = 2 * (layer_selected - 1)\n",
    "        bias_idx = weight_idx + 1\n",
    "        selected_params = [final_local_parameters[weight_idx], final_local_parameters[bias_idx]]\n",
    "        \n",
    "        # Calculate communication cost\n",
    "        effective_upload_bytes_client = np.array([layer_selected], dtype=np.int64).nbytes\n",
    "        effective_upload_bytes_client += sum(p.nbytes for p in selected_params)\n",
    "        \n",
    "        # Prepare metrics\n",
    "        metrics = {\n",
    "            \"layer_selected\": float(layer_selected),\n",
    "            \"effective_upload_bytes\": effective_upload_bytes_client,\n",
    "            \"val_accuracy\": train_results[\"val_accuracy\"],\n",
    "            \"val_precision\": train_results[\"val_precision\"],\n",
    "            \"val_recall\": train_results[\"val_recall\"],\n",
    "            \"val_f1\": train_results[\"val_f1\"],\n",
    "            \"selection_probability\": float(probabilities[selected_layer_idx]),\n",
    "            \"layer_sensitivity\": float(layer_sensitivities[selected_layer_idx])\n",
    "        }\n",
    "        \n",
    "        # Add detailed sensitivity information for analysis\n",
    "        for l, s in enumerate(sensitivity_scores):\n",
    "            metrics[f\"sensitivity_param_{l}\"] = float(s)\n",
    "        for l, p in enumerate(probabilities):\n",
    "            metrics[f\"probability_layer_{l+1}\"] = float(p)\n",
    "        \n",
    "        # Log results\n",
    "        client_logger.info(\n",
    "            f\"[Round {round_num}] Client {self.rank} XFL-S training complete - \"\n",
    "            f\"Val Acc: {train_results['val_accuracy']:.4f}, \"\n",
    "            f\"Selected Layer: {layer_selected} (prob: {probabilities[selected_layer_idx]:.4f})\"\n",
    "        )\n",
    "        \n",
    "        # Return payload: [layer_index, weight_array, bias_array]\n",
    "        payload = [\n",
    "            np.array([layer_selected], dtype=np.int64),\n",
    "            *selected_params\n",
    "        ]\n",
    "        \n",
    "        return payload, len(self.trainloader.dataset), metrics\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        parameters: List[np.ndarray],\n",
    "        config: Dict\n",
    "    ) -> Tuple[float, int, Dict]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on validation data.\n",
    "        \n",
    "        Args:\n",
    "            parameters: Global model parameters\n",
    "            config: Configuration dictionary\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (loss, num_examples, metrics)\n",
    "        \"\"\"\n",
    "        set_weights(self.net, parameters)\n",
    "        loss, accuracy, precision, recall, f1 = test(self.net, self.valloader, self.device)\n",
    "        \n",
    "        return float(loss), len(self.valloader.dataset), {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"eval_loss\": float(loss),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"XFL-S Client implementation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f77ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XFL-S STRATEGY (SERVER-SIDE AGGREGATION)\n",
    "\n",
    "# Set up server-side logging\n",
    "strategy_logger = logging.getLogger(\"XFLSStrategy\")\n",
    "strategy_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class XFLSStrategy(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    XFL-S Strategy for server-side aggregation.\n",
    "    \n",
    "    This strategy aggregates layer-wise updates from clients based on\n",
    "    sensitivity-guided selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        initial_parameters: Parameters,\n",
    "        param_keys: List[str],\n",
    "        fraction_fit: float = 1.0,\n",
    "        fraction_evaluate: float = 0.0,\n",
    "        min_fit_clients: int = 2,\n",
    "        min_evaluate_clients: int = 0,\n",
    "        min_available_clients: int = 2,\n",
    "        evaluate_fn: Optional[Callable] = None,\n",
    "        evaluate_metrics_aggregation_fn: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            fraction_fit=fraction_fit,\n",
    "            fraction_evaluate=fraction_evaluate,\n",
    "            min_fit_clients=min_fit_clients,\n",
    "            min_evaluate_clients=min_evaluate_clients,\n",
    "            min_available_clients=min_available_clients,\n",
    "            initial_parameters=initial_parameters,\n",
    "            evaluate_fn=evaluate_fn,\n",
    "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
    "        )\n",
    "        self.param_keys = param_keys\n",
    "        self.global_state = OrderedDict(\n",
    "            zip(self.param_keys, parameters_to_ndarrays(initial_parameters))\n",
    "        )\n",
    "        \n",
    "        # Track statistics\n",
    "        self.layer_selection_history = defaultdict(list)\n",
    "        self.communication_savings = []\n",
    "\n",
    "    def configure_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        parameters: Parameters,\n",
    "        client_manager\n",
    "    ) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Configure clients for training round.\n",
    "        \n",
    "        Args:\n",
    "            server_round: Current round number\n",
    "            parameters: Global model parameters\n",
    "            client_manager: Client manager\n",
    "        \n",
    "        Returns:\n",
    "            List of (client, FitIns) tuples\n",
    "        \"\"\"\n",
    "        clients = client_manager.sample(num_clients=self.min_fit_clients)\n",
    "        clients = sorted(clients, key=lambda c: c.cid)\n",
    "        \n",
    "        fit_ins = []\n",
    "        for rank, client in enumerate(clients):\n",
    "            config = {\n",
    "                \"round\": server_round,\n",
    "                \"rank\": rank,\n",
    "            }\n",
    "            fit_ins.append((client, fl.common.FitIns(parameters, config)))\n",
    "        \n",
    "        strategy_logger.info(\n",
    "            f\"[Round {server_round}] Configured {len(fit_ins)} clients for XFL-S layer selection\"\n",
    "        )\n",
    "        return fit_ins\n",
    "\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple],\n",
    "        failures: List[Tuple]\n",
    "    ) -> Tuple[Parameters, Dict]:\n",
    "        \"\"\"\n",
    "        Aggregate layer-wise updates from clients.\n",
    "        \n",
    "        Args:\n",
    "            server_round: Current round number\n",
    "            results: List of (client, FitRes) tuples\n",
    "            failures: List of failed clients\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (aggregated_parameters, metrics)\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return ndarrays_to_parameters(list(self.global_state.values())), {}\n",
    "\n",
    "        # Track layer updates: {layer_idx: [(weight, bias, num_examples), ...]}\n",
    "        layer_updates = {}\n",
    "        \n",
    "        # Tracking variables\n",
    "        effective_upload_bytes_total = 0\n",
    "        training_accuracies = []\n",
    "        training_precisions = []\n",
    "        training_recalls = []\n",
    "        training_f1s = []\n",
    "        selection_probabilities = []\n",
    "        layer_selections = []\n",
    "\n",
    "        # Process each client's result\n",
    "        for _, fit_res in results:\n",
    "            client_payload = parameters_to_ndarrays(fit_res.parameters)\n",
    "            num_examples = fit_res.num_examples\n",
    "\n",
    "            # Extract payload: [layer_index, weight_array, bias_array]\n",
    "            layer_idx_arr = client_payload[0]\n",
    "            weight_param = client_payload[1]\n",
    "            bias_param = client_payload[2]\n",
    "            layer_idx = int(layer_idx_arr[0])\n",
    "            \n",
    "            # Get selection probability\n",
    "            probability = fit_res.metrics.get(\"selection_probability\", 1.0)\n",
    "            \n",
    "            strategy_logger.info(\n",
    "                f\"[Round {server_round}] Client selected layer {layer_idx} (prob: {probability:.3f})\"\n",
    "            )\n",
    "\n",
    "            # Group updates by layer\n",
    "            if layer_idx not in layer_updates:\n",
    "                layer_updates[layer_idx] = []\n",
    "            layer_updates[layer_idx].append(((weight_param, bias_param), num_examples))\n",
    "\n",
    "            # Collect statistics\n",
    "            if \"effective_upload_bytes\" in fit_res.metrics:\n",
    "                effective_upload_bytes_total += fit_res.metrics[\"effective_upload_bytes\"]\n",
    "            \n",
    "            # Training metrics\n",
    "            if \"val_accuracy\" in fit_res.metrics:\n",
    "                training_accuracies.append(fit_res.metrics[\"val_accuracy\"])\n",
    "            if \"val_precision\" in fit_res.metrics:\n",
    "                training_precisions.append(fit_res.metrics[\"val_precision\"])\n",
    "            if \"val_recall\" in fit_res.metrics:\n",
    "                training_recalls.append(fit_res.metrics[\"val_recall\"])\n",
    "            if \"val_f1\" in fit_res.metrics:\n",
    "                training_f1s.append(fit_res.metrics[\"val_f1\"])\n",
    "            \n",
    "            selection_probabilities.append(probability)\n",
    "            layer_selections.append(layer_idx)\n",
    "\n",
    "        # Weighted aggregation for selected layers\n",
    "        for layer_idx, updates in layer_updates.items():\n",
    "            total_examples = sum(n for _, n in updates)\n",
    "            \n",
    "            # Weighted average\n",
    "            weighted_weight_sum = sum(w[0] * n for w, n in updates)\n",
    "            weighted_bias_sum = sum(w[1] * n for w, n in updates)\n",
    "            avg_weight = weighted_weight_sum / total_examples\n",
    "            avg_bias = weighted_bias_sum / total_examples\n",
    "            \n",
    "            # Update global state\n",
    "            weight_pos = 2 * (layer_idx - 1)\n",
    "            bias_pos = weight_pos + 1\n",
    "            weight_key = self.param_keys[weight_pos]\n",
    "            bias_key = self.param_keys[bias_pos]\n",
    "            \n",
    "            self.global_state[weight_key] = avg_weight\n",
    "            self.global_state[bias_key] = avg_bias\n",
    "\n",
    "            strategy_logger.info(\n",
    "                f\"[Round {server_round}] Updated layer {layer_idx} with {len(updates)} client updates\"\n",
    "            )\n",
    "\n",
    "        # Track statistics\n",
    "        self.layer_selection_history[server_round] = layer_selections\n",
    "        \n",
    "        # Calculate communication efficiency\n",
    "        total_model_size_bytes = sum(param.nbytes for param in self.global_state.values())\n",
    "        download_bytes_total = total_model_size_bytes * len(results)\n",
    "        total_bytes_effective = effective_upload_bytes_total + download_bytes_total\n",
    "        \n",
    "        # Calculate theoretical full upload\n",
    "        theoretical_full_upload = total_model_size_bytes * len(results)\n",
    "        communication_reduction = 1 - (effective_upload_bytes_total / theoretical_full_upload)\n",
    "        self.communication_savings.append(communication_reduction)\n",
    "\n",
    "        # Convert to MB\n",
    "        effective_upload_mb = effective_upload_bytes_total / (1024 ** 2)\n",
    "        download_mb = download_bytes_total / (1024 ** 2)\n",
    "        total_mb_effective = total_bytes_effective / (1024 ** 2)\n",
    "        theoretical_upload_mb = theoretical_full_upload / (1024 ** 2)\n",
    "\n",
    "        # Calculate average training metrics\n",
    "        avg_training_metrics = {}\n",
    "        if training_accuracies:\n",
    "            avg_training_metrics[\"avg_train_accuracy\"] = np.mean(training_accuracies)\n",
    "        if training_precisions:\n",
    "            avg_training_metrics[\"avg_train_precision\"] = np.mean(training_precisions)\n",
    "        if training_recalls:\n",
    "            avg_training_metrics[\"avg_train_recall\"] = np.mean(training_recalls)\n",
    "        if training_f1s:\n",
    "            avg_training_metrics[\"avg_train_f1\"] = np.mean(training_f1s)\n",
    "\n",
    "        # Prepare metrics\n",
    "        metrics = {\n",
    "            \"upload_MB\": effective_upload_mb,\n",
    "            \"download_MB\": download_mb,\n",
    "            \"total_update_MB\": total_mb_effective,\n",
    "            \"theoretical_upload_MB\": theoretical_upload_mb,\n",
    "            \"communication_reduction\": communication_reduction,\n",
    "            \"avg_selection_probability\": np.mean(selection_probabilities) if selection_probabilities else 0,\n",
    "            \"layer_diversity\": len(set(layer_selections)),\n",
    "            **avg_training_metrics,\n",
    "        }\n",
    "\n",
    "        # Enhanced logging\n",
    "        strategy_logger.info(\n",
    "            f\"[Round {server_round}]  XFL-S Communication - \"\n",
    "            f\"Upload: {effective_upload_mb:.4f} MB (vs {theoretical_upload_mb:.4f} MB full), \"\n",
    "            f\"Reduction: {communication_reduction:.1%}, \"\n",
    "            f\"Avg Probability: {np.mean(selection_probabilities):.3f}, \"\n",
    "            f\"Layer Diversity: {len(set(layer_selections))}/{max(layer_selections) if layer_selections else 0}\"\n",
    "        )\n",
    "\n",
    "        if avg_training_metrics:\n",
    "            strategy_logger.info(\n",
    "                f\"[Round {server_round}] Training Metrics - \"\n",
    "                f\"Avg Accuracy: {avg_training_metrics.get('avg_train_accuracy', 0):.4f}, \"\n",
    "                f\"Avg F1: {avg_training_metrics.get('avg_train_f1', 0):.4f}\"\n",
    "            )\n",
    "\n",
    "        return ndarrays_to_parameters(list(self.global_state.values())), metrics\n",
    "\n",
    "\n",
    "print(\"XFL-S Strategy implementation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc21f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER FUNCTIONS\n",
    "\n",
    "def load_centralized_test_data() -> DataLoader:\n",
    "    \"\"\"\n",
    "    Load centralized test dataset for server-side evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader for centralized test data\n",
    "    \"\"\"\n",
    "    partitioner = IidPartitioner(num_partitions=1)\n",
    "    fds_centralized = FederatedDataset(\n",
    "        dataset=\"uoft-cs/cifar10\",\n",
    "        partitioners={\"train\": partitioner},\n",
    "    )\n",
    "    partition = fds_centralized.load_partition(0)\n",
    "    partition_test = partition.train_test_split(test_size=0.3, seed=42)[\"test\"]\n",
    "    \n",
    "    pytorch_transforms = Compose([\n",
    "        ToTensor(), \n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "        return batch\n",
    "\n",
    "    partition_test = partition_test.with_transform(apply_transforms)\n",
    "    centralized_testloader = DataLoader(\n",
    "        partition_test, batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    print(f\"Centralized test dataset size: {len(partition_test)}\")\n",
    "    return centralized_testloader\n",
    "\n",
    "\n",
    "def gen_evaluate_fn(testloader: DataLoader, device: torch.device) -> Callable:\n",
    "    \"\"\"\n",
    "    Generate evaluation function for centralized testing.\n",
    "    \n",
    "    Args:\n",
    "        testloader: Test data loader\n",
    "        device: Device to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation function\n",
    "    \"\"\"\n",
    "    def evaluate(server_round: int, parameters_ndarrays: List[np.ndarray], config: Dict):\n",
    "        print(f\"[Server] Starting centralized evaluation for round {server_round}\")\n",
    "        net = get_model(MODEL_TYPE)\n",
    "        set_weights(net, parameters_ndarrays)\n",
    "        net.to(device)\n",
    "        loss, accuracy, precision, recall, f1 = test(net, testloader, device)\n",
    "        \n",
    "        print(f\"[Server] XFL-S Centralized Evaluation Round {server_round}:\")\n",
    "        print(f\"  - Loss: {loss:.4f}\")\n",
    "        print(f\"  - Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  - Precision: {precision:.4f}\")\n",
    "        print(f\"  - Recall: {recall:.4f}\")\n",
    "        print(f\"  - F1-Score: {f1:.4f}\")\n",
    "        \n",
    "        return loss, {\n",
    "            \"centralized_accuracy\": accuracy,\n",
    "            \"centralized_precision\": precision,\n",
    "            \"centralized_recall\": recall,\n",
    "            \"centralized_f1\": f1,\n",
    "            \"centralized_loss\": loss,\n",
    "        }\n",
    "    return evaluate\n",
    "\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    \"\"\"\n",
    "    Aggregate metrics using weighted average.\n",
    "    \n",
    "    Args:\n",
    "        metrics: List of (num_examples, metrics_dict) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Aggregated metrics dictionary\n",
    "    \"\"\"\n",
    "    if not metrics:\n",
    "        return {}\n",
    "    \n",
    "    metric_keys = list(metrics[0][1].keys())\n",
    "    aggregated_metrics = {}\n",
    "    \n",
    "    for key in metric_keys:\n",
    "        weighted_values = [num_examples * m.get(key, 0) for num_examples, m in metrics]\n",
    "        examples = [num_examples for num_examples, _ in metrics]\n",
    "        \n",
    "        if sum(examples) > 0:\n",
    "            aggregated_metrics[key] = sum(weighted_values) / sum(examples)\n",
    "        else:\n",
    "            aggregated_metrics[key] = 0.0\n",
    "    \n",
    "    return aggregated_metrics\n",
    "\n",
    "\n",
    "print(\"Server functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIENT APP FUNCTION\n",
    "\n",
    "def client_fn(context: Context) -> XFLSClient:\n",
    "    \"\"\"\n",
    "    Create a client instance for federated learning.\n",
    "    \n",
    "    Args:\n",
    "        context: Client context\n",
    "    \n",
    "    Returns:\n",
    "        XFLSClient instance\n",
    "    \"\"\"\n",
    "    pid = context.node_config[\"partition-id\"]\n",
    "    cid = context.node_id\n",
    "    nps = context.node_config[\"num-partitions\"]\n",
    "    \n",
    "    trainloader, valloader = load_data(pid, nps, BATCH_SIZE)\n",
    "    net = get_model(MODEL_TYPE)\n",
    "    \n",
    "    return XFLSClient(\n",
    "        net, trainloader, valloader, LOCAL_EPOCHS, LEARNING_RATE, rank=pid, cid=cid\n",
    "    ).to_client()\n",
    "\n",
    "\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c579e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SERVER APP FUNCTION \n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    \"\"\"\n",
    "    Create server components for federated learning.\n",
    "    \n",
    "    Args:\n",
    "        context: Server context\n",
    "    \n",
    "    Returns:\n",
    "        ServerAppComponents with strategy and config\n",
    "    \"\"\"\n",
    "    # Setup centralized evaluation\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    centralized_testloader = load_centralized_test_data()\n",
    "    model = get_model(MODEL_TYPE)\n",
    "    ndarrays = get_weights(model)\n",
    "    parameters = ndarrays_to_parameters(ndarrays)\n",
    "    param_keys = list(model.state_dict().keys())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"XFL-S: eXtreme Federated Learning guided by Sensitivity\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Mathematical Formulation:\")\n",
    "    print(\"  1. Δ_{t,l} = (1/t) × Σ(w_{e,l}^local - w_{e-1,l}^local)\")\n",
    "    print(\"  2. s_{t,l} = ||Δ_{t,l}|| / ||x_{t,l}||\")\n",
    "    print(\"  3. p_{t,l} = s_{t,l} / Σ(s_{t,k})\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Expected Benefits:\")\n",
    "    print(\"  - Sensitivity-based layer selection\")\n",
    "    print(\"  - Reduced communication overhead\")\n",
    "    print(\"  - Maintained model accuracy\")\n",
    "    print(\"  - Adaptive to data heterogeneity\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    strategy = XFLSStrategy(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=FRACTION_EVALUATE,\n",
    "        min_fit_clients=NUM_CLIENTS,\n",
    "        min_evaluate_clients=NUM_CLIENTS,\n",
    "        min_available_clients=NUM_CLIENTS,\n",
    "        initial_parameters=parameters,\n",
    "        param_keys=param_keys,\n",
    "        evaluate_fn=gen_evaluate_fn(centralized_testloader, device),\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "    )\n",
    "    \n",
    "    config = ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "\n",
    "server = ServerApp(server_fn=server_fn)\n",
    "print(\"Client and Server apps configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143258bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESOURCE CONFIGURATION\n",
    "# Configure computational resources\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 2, \"num_gpus\": 0.0}}\n",
    "if device == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 4, \"num_gpus\": 0.5}}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Resource Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  - CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  - CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  - Current Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"  - Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  - GPU Memory per Client: 0.5 GPU\")\n",
    "else:\n",
    "    print(\"  - Using CPU for training\")\n",
    "    print(f\"  - CPU Cores per Client: 2\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN SIMULATION FUNCTION\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the XFL-S simulation.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING XFL-S SIMULATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Configuration Summary:\")\n",
    "    print(f\"  - Scenario: {scenario_config['name']}\")\n",
    "    print(f\"  - Algorithm: XFL-S (eXtreme Federated Learning guided by Sensitivity)\")\n",
    "    print(f\"  - Model: {MODEL_TYPE}\")\n",
    "    print(f\"  - Data Distribution: {DATA_DISTRIBUTION.upper()}\")\n",
    "    print(f\"  - Clients: {NUM_CLIENTS}\")\n",
    "    print(f\"  - Rounds: {NUM_ROUNDS}\")\n",
    "    print(f\"  - Local Epochs (t): {LOCAL_EPOCHS}\")\n",
    "    print(f\"  - Dataset: CIFAR-10\")\n",
    "    print(f\"  - Device: {device.upper()}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Run the simulation\n",
    "    run_simulation(\n",
    "        server_app=server,\n",
    "        client_app=client,\n",
    "        num_supernodes=NUM_CLIENTS,\n",
    "        backend_config=backend_config,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"XFL-S SIMULATION COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Key Metrics to Analyze:\")\n",
    "    print(\"  - Communication Reduction\")\n",
    "    print(\"  - Model Accuracy\")\n",
    "    print(\"  - Layer Selection Distribution\")\n",
    "    print(\"  - Sensitivity Score Analysis\")\n",
    "    print(\"  - Probability Distribution Effectiveness\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Author: Khaji Sana\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRY POINT\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
